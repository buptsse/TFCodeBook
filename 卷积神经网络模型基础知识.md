####卷积神经网络

在前面的神经网络,才用的是典型的full-connected:网络中的神经元与相邻的层上的每个神经元均连接

![](https://xhhjin.gitbooks.io/neural-networks-and-deep-learning-zh/content/assets/tikz41.png)



使用全连接层的网络来分类图像是很奇怪的。原因是这样的一个网络架构不考虑图像的空间结构
它在完全相同的基础上去对待相距很远和彼此接近的输入像素。这样的空间结构的概念必须从训练数据中推断。但是如果我们使用一个设法利用空间结构的架构，而不是从一个白板状态的网络架构开始，会怎样？

这次我们来看看用卷积神经网络效果如何.
卷积神经网络采用了三种基本概念：
* 局部感受野（local receptive fields），
* 共享权重（shared weights）
* 混合（pooling)

##### 局部感受野
这次不把每个输入像素连接到每个隐藏神经元。相反，我们只是把输入图像进行小的，局部区域的连接.
对于每个局部感受野，在第一个隐藏层中有一个不同的隐藏神经元。相当于做了一个局部的卷积.
之前一直把卷积层和隐藏层搞混淆了,最后总算明白了他俩之间的关系.
神经网络的通用架构就是 input layer -> hidden layer ->  output layer
卷积层只是用来描述层与层之间的映射关系,层与层之间的关系在传统神经网络里是full-connect的映射.
上层输入的数据经过卷积之后,可以看成是特征的匹配,有另外一种说法,就是把卷积看成了过滤器(filter),剥离出他关注的东西.
上层的数据可能包含多个通道数据,经过卷积之后,形成对应的数据.
下图这种画法,对于新手,在可视化层面上更像是一个二维数据的输入(只有一个通道的数据),然后做了一次卷积.
但是当你了解了足够多的cnn资料之后,你会发现左边的输入层数据是一个三维数据(多通道的数据),做了一次卷积之后,形成了一个二维数据.
也就是如果我们的输入数据是一个多通道的数据,这些多通道数据会和卷积核进行运算,每运算一次,就会形成对应的通道数据.
有多少个卷积核,就会产生多个通道的数据.这个时候隐藏层的神经元就是一个二维形态,而不仅仅只有只有一列了.

![](http://upload-images.jianshu.io/upload_images/3352761-865848c9ee8ab86d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://xhhjin.gitbooks.io/neural-networks-and-deep-learning-zh/content/assets/tikz45.png)

另一方面,我们看到上图的跨距是1,也就是每次都是移动1的距离来匹配特征,其实在实现过程中跨距不一定是1, 也可以是2等等.

##### 共享权重和偏置
隐藏神经元中的同一个channel中的每一个使用相同的权重和偏置.
这意味着隐藏层中同一个channel的所有神经元检测完全相同的特征.
卷积网络能很好地适应图像的平移不变性.
因为这个原因，我们有时候把从输入层到隐藏层的映射称为一个特征映射。我们把定义特征映射的权重称为共享权重。我们把以这种方式定义特征映射的偏置称为共享偏置。共享权重和偏置经常被称为一个卷积核或者滤波器。

目前上面俩张图描述的网络结构只能检测一种局部特征的类型。为了完成图像识别我们需要超过一个的特征映射。所以一个完整的卷积层由几个不同的特征映射组成.
![](http://upload-images.jianshu.io/upload_images/3352761-08960aa725a561c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

共享权重和偏置的一个很大的优点是，它大大减少了参与的卷积网络的参数。直观地看,这有利于我们更快完成训练建立模型.

#####混合层

除了刚刚描述的卷积层，卷积神经网络也包含混合层（pooling layers）。混合层通常紧接着在卷积层之后使用。
它要做的是简化从卷积层输出的信息,把信息简化.
常用的有max-pooling以及L2 pooling.
![](http://upload-images.jianshu.io/upload_images/3352761-55df7841a1083239.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
卷积神经网络的输入层就是图像的原始像素，而输出层中的每一个节点代表了不同类别的可信度。
![](http://upload-images.jianshu.io/upload_images/3352761-e4201d93e3b2de50?imageMogr2/auto-orient/strip)


好了,分析一下Alex的CNN结构图,看看你能不能看懂
![](http://upload-images.jianshu.io/upload_images/3352761-af96351753130f1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

输入：224×224大小的图片，3通道
针对input_layer的输入数据,我们采用了一个11*11的卷积核,跨距设置为4.但是你会发现初始位置以步长4为单位是无法刚好滑倒末尾位置.
这个时候我们需要对它做四周一下zero padding(填充),使得滑动窗口可以恰好停留到末尾.计算了一下需要增加3行,3列的zero padding.
增加了之后,经过卷积形成55*55单通道的数据

第一层卷积：5×5大小的卷积核96个，每个GPU上48个。
第一层max-pooling：2×2的核。经过卷积和池化以后,我们输出的数据是一个27*27的单通道数据
第二层卷积：3×3卷积核256个，每个GPU上128个。
第二层max-pooling：2×2的核。
第三层卷积：与上一层是全连接，3*3的卷积核384个。分到两个GPU上个192个。
第四层卷积：3×3的卷积核384个，两个GPU各192个。该层与上一层连接没有经过pooling层。
第五层卷积：3×3的卷积核256个，两个GPU上个128个。
第五层max-pooling：2×2的核。
第一层全连接：4096维，将第五层max-pooling的输出连接成为一个一维向量，作为该层的输入。
第二层全连接：4096维
Softmax层：输出为1000，输出的每一维都是图片属于该类别的概率。


Refer
[TensorFlow学习笔记2：构建CNN模型
](http://www.jeyzhang.com/tensorflow-learning-notes-2.html)
[Neural Networks and Deep Learning 英文版](http://neuralnetworksanddeeplearning.com/index.html)
[Deep Learning (Adaptive Computation and Machine Learning series)](http://www.deeplearningbook.org/)
[卷积神经](https://xhhjin.gitbooks.io/neural-networks-and-deep-learning-zh/content/chap6-1.html)
[技术向：一文读懂卷积神经网络CNN](http://dataunion.org/11692.html)
[深度 | 从入门到精通：卷积神经网络初学者指南（附论文）](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650717691&idx=2&sn=3f0b66aa9706aae1a30b01309aa0214c#rd)

[CNN笔记：通俗理解卷积神经网络](http://blog.csdn.net/v_july_v/article/details/51812459#comments)
[[透析] 卷积神经网络CNN究竟是怎样一步一步工作的？](http://www.jianshu.com/p/fe428f0b32c1)
[Conv Nets: A Modular Perspective](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)
[Understanding Convolutions](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)